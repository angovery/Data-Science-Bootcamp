{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS DATAFRAME 'DIAMONDS' CON PYSPARK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJETIVO DEL ESTUDIO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>El objetivo del estudio es crear un modelo que sea capaz de predecir el precio de un diamante.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.-CARGA DE LIBRERÍAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, NumericType, IntegerType\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.ml.feature import StringIndexer, Imputer, OneHotEncoder, VectorAssembler, RobustScaler, MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.-CREACIÓN SESIÓN PYSPARK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"pipeline_diamonds\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.-CARGA DEL DATAFRAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|326.0|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|326.0|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|327.0|4.05|4.07|2.31|\n",
      "| 0.29|Premium|    I|    VS2| 62.4| 58.0|334.0| 4.2|4.23|2.63|\n",
      "| 0.31|   Good|    J|    SI2| 63.3| 58.0|335.0|4.34|4.35|2.75|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- carat: float (nullable = true)\n",
      " |-- cut: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- clarity: string (nullable = true)\n",
      " |-- depth: float (nullable = true)\n",
      " |-- table: float (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- z: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/diamonds.csv' # Se define la URL del archivo csv.\n",
    "csv_path = 'diamonds.csv'           # Se define el nombre del archivo csv.\n",
    "\n",
    "with open(csv_path, 'wb') as file:                  # Se descarga el archivo csv.\n",
    "    file.write(requests.get(url).content)\n",
    "    \n",
    "schema = StructType([                               # Se define el Schema del DataFrame.\n",
    "    StructField('carat', FloatType(), True),\n",
    "    StructField('cut', StringType(), True),\n",
    "    StructField('color', StringType(), True),\n",
    "    StructField('clarity', StringType(), True),\n",
    "    StructField('depth', FloatType(), True),\n",
    "    StructField('table', FloatType(), True),\n",
    "    StructField('price', FloatType(), True),\n",
    "    StructField('x', FloatType(), True),\n",
    "    StructField('y', FloatType(), True),\n",
    "    StructField('z', FloatType(), True)\n",
    "])\n",
    "\n",
    "# Se lee el archivo csv con el Schema definido.\n",
    "df = spark.read.csv(csv_path,\n",
    "                    header=True,        # Se indica que la primera fila es el encabezado.\n",
    "                    inferSchema=False,  # Se indica que no se infiera el tipo de dato de cada columna.\n",
    "                    schema=schema       # Se indica el Schema definido.\n",
    "                    )\n",
    "df.show(5)          # Se muestran los primeros 5 registros del DataFrame.\n",
    "df.printSchema()    # Se muestra el Schema del DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.-PRE-PROCESADOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.-Preparación previa del DataFRame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.-Eliminación de valores 'NaN' en la columna objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La variable objetivo es 'precio'. Por tanto, se eliminan las filas con valores nulos en esta variable.\n",
    "df_regresion = df.dropna(subset=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La variable objetivo es 'cut'. Por tanto, se eliminan las filas con valores nulos en esta variable.\n",
    "df_clasificacion = df.dropna(subset=['cut'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.-Contabilizar valores 'NaN' en las columnas del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|carat|cut|color|clarity|depth|table|price|  x|  y|  z|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|    0|  0|    0|      0|    0|    0|    0|  0|  0|  0|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se cuentan los valores nulos en cada columna.\n",
    "df_regresion.select([sum(col(c).isNull().cast('int')).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|carat|cut|color|clarity|depth|table|price|  x|  y|  z|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "|    0|  0|    0|      0|    0|    0|    0|  0|  0|  0|\n",
      "+-----+---+-----+-------+-----+-----+-----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se cuentan los valores nulos en cada columna.\n",
    "df_clasificacion.select([sum(col(c).isNull().cast('int')).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Aunque ninguna de las columnas presentan valores nulos, se va a considerar (a efectos de ejemplo del ejercicio) que sí los tiene.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.-REGRESIÓN DE LA COLUMNA NUMÉRICA 'price'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Dado que el ejercicio se compone de dos partes bien diferenciadas: regresión de la columna 'price' y clasificación de la columna 'carat', y para evitar posibles problemas con la fuente de datos, se crea una copia del DataFrame por cada tipo de modelado.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.-Separación de las columnas numéricas, no-numéricas y variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se seleccionan los nombres de las columnas a las que aplicar Preprocesados.\n",
    "num_cols = [field.name for field in df_regresion.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "cat_cols = [field.name for field in df_regresion.schema.fields if isinstance(field.dataType, StringType) and field.name != 'price']\n",
    "label_col = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.-Pre-Procesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.-Indexados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.1.-Indexado de la columna objetivo: 'price'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se indexa la columna a predecir 'price' como columna objetivo ('label').\n",
    "indexer_label = StringIndexer(\n",
    "    inputCol=label_col,         # Nombre de la columna objetivo a indexar.\n",
    "    outputCol='label',          # Nombre de salida de la columna indexada.\n",
    "    handleInvalid='keep'        # Se mantienen los valores no vistos en el entrenamiento.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.2.-Indexado de la columnas categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed', 'color_indexed', 'clarity_indexed']\n"
     ]
    }
   ],
   "source": [
    "# Se indexan las columnas categóricas de la entrada (features) que no son la columna label a predecir.\n",
    "# Se crea un objeto StringIndexer por cada columna categórica a indexar.\n",
    "indexers_features = [\n",
    "    StringIndexer(inputCol=c,                   # Nombre de la columna categórica a indexar.\n",
    "                  outputCol=c + '_indexed',     # Nombre de salida de la columna indexada.\n",
    "                  handleInvalid='keep'          # Se mantienen los valores no vistos en el entrenamiento.\n",
    "                  ) for c in cat_cols\n",
    "]\n",
    "cat_cols_indexed = [c + '_indexed' for c in cat_cols]   # Se crea una lista con los nombres de las columnas indexadas.\n",
    "print(cat_cols_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.-Imputers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2.1.-Imputers de variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'price_imputed', 'x_imputed', 'y_imputed', 'z_imputed']\n"
     ]
    }
   ],
   "source": [
    "imputer_numericas = Imputer(\n",
    "    inputCols=num_cols,                                 # Nombre de las columnas numéricas a imputar.\n",
    "    outputCols=[c + '_imputed' for c in num_cols],      # Nombre de salida de las columnas imputadas\n",
    "    strategy='median'                                   # Se imputa con la mediana.\n",
    ")\n",
    "num_cols_imputed = [c + '_imputed' for c in num_cols]   # Se crea una lista con los nombres de las columnas imputadas.\n",
    "print(num_cols_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2.2.-Imputers de variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed_imputed', 'color_indexed_imputed', 'clarity_indexed_imputed']\n"
     ]
    }
   ],
   "source": [
    "imputer_categoricas = Imputer(\n",
    "    inputCols=cat_cols_indexed,                             # Nombre de las columnas categóricas a imputar.\n",
    "    outputCols=[c + '_imputed' for c in cat_cols_indexed],  # Nombre de salida de las columnas imputadas.\n",
    "strategy='mode'                                             # Se imputa con la moda.\n",
    ")\n",
    "cat_cols_indexed_imputed = [c + '_imputed' for c in cat_cols_indexed]   # Se crea una lista con los nombres de las columnas imputadas.\n",
    "print(cat_cols_indexed_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2.3.-Codificación de variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cut_indexed_imputed_onehot', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "encoder_onehot = [\n",
    "    OneHotEncoder(inputCol=c, outputCol=c + '_onehot') \n",
    "    for c in cat_cols_indexed_imputed\n",
    "]\n",
    "cat_cols_onehot = [c + '_onehot' for c in cat_cols_indexed_imputed]\n",
    "print(cat_cols_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3.-Escalados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>En el ejercicio se va a utilizar el algoritmo RandomForestRegressor para generar el modelo de regresión. Este algoritmo, al no estar basado distancias, no es tan sensible a las diferentes escalas de los valores de las variables. No obstante, a modo de ejemplo, y de cara a realizar un ejercicio más completo, se va a efectuar un escalado.</p>\n",
    "<p>El método del escalado que se va a emplear es RobustScaler (en lugar de lo indicado en el enunciado: MinMaxScaler o StandardScaler), debido a que, por lo desarrollado en el ejercicio del módulo 3, en el DataFrame de 'Diamonds' existen columnas con distribuciones muy asimétricas y con presencia de numerosos outliers. RobustScaler es más apropiado para estos casos, ya que hace uso de la mediana y los rangos inter-cuartílicos.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3.1-Ensamblado de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Antes de aplicar el escalado, se aplica el ensamblado de las variables para unificar ambos tipos de variables, numéricas y categóricas, en un único vector.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'price_imputed', 'x_imputed', 'y_imputed', 'z_imputed', 'cut_indexed_imputed_onehot', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Se crea una lista con el nombre de las columnas a ensamblar, que seran las columnas 'features'.\n",
    "col_ensambladas = [col for col in num_cols_imputed] + \\\n",
    "                         [col for col in cat_cols_onehot]\n",
    "print(col_ensambladas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ensamblan los nombres de las columnas numéricas y categóricas en una única columna 'features'.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=col_ensambladas,      # Nombre de las columnas a ensamblar.\n",
    "    outputCol='features'            # Nombre de salida de la columna ensamblada.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3.2-Escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de todas las 'features' utilizando RobustScaler.\n",
    "scaler = RobustScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.-Modelado, Pipelines y Evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1.-Generación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Como se ha mencionado anteriormente, para la regresión se utiliza el algoritmo RandomForestRegressor.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eligen parámetros de ejemplo (100 árboles, profundidad máxima de 5)\n",
    "model_regresion = RandomForestRegressor(featuresCol=\"scaled_features\",   # Vector de características.\n",
    "                                    labelCol=label_col,                 # Columna objetivo.\n",
    "                                        numTrees=100,                   # Número de árboles.\n",
    "                                        maxDepth=5                      # Profundidad máxima.\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2.-Particionamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg_train, df_reg_test = df_regresion.randomSplit([0.8, 0.2], seed=42)  # Se divide el DataFrame en Train y Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3.-Generación del Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_regresion = Pipeline(stages = [\n",
    "    indexer_label,          # Indexador de la columna objetivo.\n",
    "    *indexers_features,     # Indexadores de las columnas categóricas. (se coloca '*' para desempaquetar la lista de objetos).\n",
    "    imputer_numericas,      # Imputador de las columnas numéricas.\n",
    "    imputer_categoricas,    # Imputador de las columnas categóricas.\n",
    "    *encoder_onehot,        # OneHotEncoder de las columnas categóricas. (se coloca '*' para desempaquetar la lista de objetos).\n",
    "    assembler,              # Ensamblador de las columnas numéricas y categóricas.\n",
    "    scaler,                 # Escalador de las 'features'.\n",
    "    model_regresion         # Modelo de Regresión.\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4.-Entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = pipeline_regresion.fit(df_reg_train)  # Se entrena el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.5.-Predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion_reg = model_reg.transform(df_reg_test)  # Se realiza la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.6.-Evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluación del Modelo de Regresión (RandomForestRegressor con RobustScaler) ===\n",
      "R2: 0.9773520052305722\n",
      "MAE: 1981.4725284922758\n",
      "MSE: 7849110.502337356\n",
      "RMSE: 607.0337566183553\n"
     ]
    }
   ],
   "source": [
    "evaluador_r2 = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='r2')\n",
    "r2 = evaluador_r2.evaluate(prediccion_reg)\n",
    "evaluador_mae = RegressionEvaluator(metricName='mae')\n",
    "mae = evaluador_mae.evaluate(prediccion_reg) \n",
    "evaluador_mse = RegressionEvaluator(metricName='mse')\n",
    "mse = evaluador_mse.evaluate(prediccion_reg)\n",
    "evaluador_rmse = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluador_rmse.evaluate(prediccion_reg)\n",
    "\n",
    "print(\"=== Evaluación del Modelo de Regresión (RandomForestRegressor con RobustScaler) ===\")\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del resultado de la evaluación del modelo de regresión podemos concluir lo siguiente:\n",
    "* R2: El valor de esta métrica indica que el 97,7% de la variabilidad en el precio se explica por el modelo. Es un valor realmente bueno, ya que casi toda la variabilidad de los datos se captura con la infdormación empleada.\n",
    "* MAE: En promedio, las predicciones se desvían del valor real en cerca de 1989.81 unidades de precio. \n",
    "* MSE: Esta métrica penaliza más fuertemente los errores grandes, ya que eleva al cuadrado cada diferencia. Un MSE elevado puede indicar la presencia de algunos errores significativamente grandes o outliers. Este valor, sin embargo, está en unidades al cuadrado, lo que dificulta una interpretación directa en el contexto del precio.\n",
    "* RMSE: Esta métrica es la raíz cuadrada del MSE. El valor 610,975$ indica que las predicciones del modelo pueden diferir en la cantidad indicada por la métrica. Este valor (610,975$), teniendo en cuenta que el valor mínimo de la columna 'price' es 326,0$, el valor máximo es 18.823,0$, media de 3.932,0$, mediana 2401,0$ y que la distribución de precios presenta numerosos outliers y tiene una clara y acusada asimetría hacia valores altos, se podría considerar que el RMSE de 610,975 $ podría ser un valor aceptable. Sin embargo, si se desea mejorar el resultado, se podría actuar sobre los siguientes factores:\n",
    "    - Tratamiento de outliers: Aunque se ha utilizado el escalado RobustScaler (recomendado ante la presencia de outliers), se podrían tratar, previas al escalado, los outliers mediante otras técnicas (Valla de Tukey, Z-Score, etc.)\n",
    "    - Transformación de los valores de la columna objetivo mediante funciones raíz o logarítmica.\n",
    "    - Ajuste de hiperparámetros del modelo.\n",
    "    - Evaluación y utilización de otros modelos de regresión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.-REGRESIÓN DE LA COLUMNA CATEGÓRICA 'cut'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Esta es la fase del modelado de clasificación de la columna 'cut'.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.-Separación de las columnas numéricas, no-numéricas y variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se seleccionan los nombres de las columnas a las que aplicar Preprocesados.\n",
    "num_cols_clas = [field.name for field in df_clasificacion.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "cat_cols_clas = [field.name for field in df_clasificacion.schema.fields if isinstance(field.dataType, StringType) and field.name != 'cut']\n",
    "label_col_clas = 'cut'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.-Pre-Procesados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1.-Indexados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1.1.-Indexado de la columna objetivo: 'cut'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se indexa la columna a predecir 'cut' como columna objetivo ('label').\n",
    "indexer_label_clas = StringIndexer(\n",
    "    inputCol=label_col_clas,        # Nombre de la columna objetivo a indexar.\n",
    "    outputCol='label',              # Nombre de salida de la columna indexada.\n",
    "    handleInvalid='keep'            # Se mantienen los valores no vistos en el entrenamiento.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1.2.-Indexado de la columnas categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed', 'clarity_indexed']\n"
     ]
    }
   ],
   "source": [
    "# Se indexan las columnas categóricas de la entrada (features) que no son la columna label a predecir.\n",
    "# Se crea un objeto StringIndexer por cada columna categórica a indexar.\n",
    "indexers_features_clas = [\n",
    "    StringIndexer(inputCol=c,                   # Nombre de la columna categórica a indexar.\n",
    "                  outputCol=c + '_indexed',     # Nombre de salida de la columna indexada.\n",
    "                  handleInvalid='keep'          # Se mantienen los valores no vistos en el entrenamiento.\n",
    "                  ) for c in cat_cols_clas\n",
    "]\n",
    "cat_cols_clas_indexed = [c + '_indexed' for c in cat_cols_clas]   # Se crea una lista con los nombres de las columnas indexadas.\n",
    "print(cat_cols_clas_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.-Imputers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.1.-Imputers de variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'price_imputed', 'x_imputed', 'y_imputed', 'z_imputed']\n"
     ]
    }
   ],
   "source": [
    "imputer_numericas_clas = Imputer(\n",
    "    inputCols=num_cols_clas,                                # Nombre de las columnas numéricas a imputar.\n",
    "    outputCols=[c + '_imputed' for c in num_cols_clas],     # Nombre de salida de las columnas imputadas\n",
    "strategy='median'                                           # Se imputa con la mediana.\n",
    ")\n",
    "num_cols_clas_imputed = [c + '_imputed' for c in num_cols_clas]   # Se crea una lista con los nombres de las columnas imputadas.\n",
    "print(num_cols_clas_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.2.-Imputers de variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed_imputed', 'clarity_indexed_imputed']\n"
     ]
    }
   ],
   "source": [
    "imputer_categoricas_clas = Imputer(\n",
    "    inputCols=cat_cols_clas_indexed,                             # Nombre de las columnas categóricas a imputar.\n",
    "    outputCols=[c + '_imputed' for c in cat_cols_clas_indexed],  # Nombre de salida de las columnas imputadas.\n",
    "strategy='mode'                                                  # Se imputa con la moda.\n",
    ")\n",
    "cat_cols_clas_indexed_imputed = [c + '_imputed' for c in cat_cols_clas_indexed]   # Se crea una lista con los nombres de las columnas imputadas.\n",
    "print(cat_cols_clas_indexed_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2.3.-Codificación de variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "encoder_onehot_clas = [\n",
    "    OneHotEncoder(inputCol=c, outputCol=c + '_onehot') \n",
    "    for c in cat_cols_clas_indexed_imputed\n",
    "]\n",
    "cat_cols_clas_onehot = [c + '_onehot' for c in cat_cols_clas_indexed_imputed]\n",
    "print(cat_cols_clas_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3.-Escalados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>En este caso, para las clasificación, se va a utilizar el algoritmo MultiLayerPerceptronClassifier para generar el modelo. Este algoritmo, sí está basado en distancias. Por tanto, es más sensible a las diferentes escalas de los valores de las variables.</p>\n",
    "<p>El método del escalado que se va a emplear es MinMaxScaler, a pesar de la presencia de outliers en el DataFrame.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.3.1-Ensamblado de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Antes de aplicar el escalado, se aplica el ensamblado de las variables para unificar ambos tipos de variables, numéricas y categóricas, en un único vector.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat_imputed', 'depth_imputed', 'table_imputed', 'price_imputed', 'x_imputed', 'y_imputed', 'z_imputed', 'color_indexed_imputed_onehot', 'clarity_indexed_imputed_onehot']\n"
     ]
    }
   ],
   "source": [
    "# Se crea una lista con el nombre de las columnas a ensamblar, que seran las columnas 'features'.\n",
    "col_ensambladas_clas = [col for col in num_cols_clas_imputed] + \\\n",
    "                         [col for col in cat_cols_clas_onehot]\n",
    "print(col_ensambladas_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se ensamblan los nombres de las columnas numéricas y categóricas en una única columna 'features'.\n",
    "assembler_clas = VectorAssembler(\n",
    "    inputCols=col_ensambladas_clas,      # Nombre de las columnas a ensamblar.\n",
    "    outputCol='features'            # Nombre de salida de la columna ensamblada.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.3.2-Escalado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalado de todas las 'features' utilizando MinMaxScaler.\n",
    "scaler_clas = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.-Modelado, Pipelines y Evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.1.-Particionamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clas_train, df_clas_test = df_clasificacion.randomSplit([0.8, 0.2], seed=42)  # Se divide el DataFrame en Train y Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.2.-Configuración del modelo MultiLayerPerceptronClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro 'layers' define la arquitectura de la red:\n",
    "- La capa de entrada debe tener el tamaño igual al número de features.\n",
    "- La capa de salida debe tener el número de clases (para \"cut\", generalmente 5 categorías).\n",
    "- Se definen capas ocultas (por ejemplo, una capa oculta con 10 neuronas).\n",
    "<p>Para determinar el tamaño de la capa de entrada, se entrena un pipeline temporal hasta el ensamblador.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de la capa de entrada (número de features): 20\n"
     ]
    }
   ],
   "source": [
    "temp_pipeline = Pipeline(stages = [\n",
    "    *indexers_features_clas,\n",
    "    imputer_categoricas_clas,\n",
    "    *encoder_onehot_clas, \n",
    "    imputer_numericas_clas,\n",
    "    assembler_clas\n",
    "    ])\n",
    "temp_model = temp_pipeline.fit(df_reg_train)  # Se puede usar el mismo train_reg\n",
    "temp_df = temp_model.transform(df_reg_train)\n",
    "first_row = temp_df.select(\"features\").first()\n",
    "input_size = len(first_row['features'])\n",
    "print(\"Tamaño de la capa de entrada (número de features):\", input_size)\n",
    "\n",
    "# Suponiendo que \"cut\" tiene 5 clases (por ejemplo: Fair, Good, Very Good, Premium, Ideal)\n",
    "layers = [input_size, 10, 5]  # [capa de entrada, capa oculta, capa de salida]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.3.-Generación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptronClassifier(featuresCol=\"scaled_features\", labelCol='label',\n",
    "                                     layers=layers, maxIter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.4.-Generación del Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_clasificacion = Pipeline(stages = [\n",
    "    indexer_label_clas,          # Indexador de la columna objetivo.\n",
    "    *indexers_features_clas,     # Indexadores de las columnas categóricas. (se coloca '*' para desempaquetar la lista de objetos).\n",
    "    imputer_numericas_clas,      # Imputador de las columnas numéricas.\n",
    "    imputer_categoricas_clas,    # Imputador de las columnas categóricas.\n",
    "    *encoder_onehot_clas,        # OneHotEncoder de las columnas categóricas. (se coloca '*' para desempaquetar la lista de objetos).\n",
    "    assembler_clas,              # Ensamblador de las columnas numéricas y categóricas.\n",
    "    scaler_clas,                 # Escalador de las 'features'.\n",
    "    mlp\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.5.-Entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_clasificacion = pipeline_clasificacion.fit(df_clas_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.6.-Predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clasificacion_transformado = modelo_clasificacion.transform(df_clas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Como se ha mencionado anteriormente, para la regresión se utiliza el algoritmo RandomForestRegressor.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3.7.-Evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluación del Modelo de Regresión (RandomForestRegressor con RobustScaler) ===\n",
      "R2: 0.9773520052305722\n",
      "MAE: 1981.4725284922758\n",
      "MSE: 7849110.502337356\n",
      "RMSE: 607.0337566183553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "evaluador_r2 = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='r2')\n",
    "r2 = evaluador_r2.evaluate(prediccion_reg)\n",
    "evaluador_mae = RegressionEvaluator(metricName='mae')\n",
    "mae = evaluador_mae.evaluate(prediccion_reg) \n",
    "evaluador_mse = RegressionEvaluator(metricName='mse')\n",
    "mse = evaluador_mse.evaluate(prediccion_reg)\n",
    "evaluador_rmse = RegressionEvaluator(labelCol=label_col, predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluador_rmse.evaluate(prediccion_reg)\n",
    "\n",
    "print(\"=== Evaluación del Modelo de Regresión (RandomForestRegressor con RobustScaler) ===\")\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
